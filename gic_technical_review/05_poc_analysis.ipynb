{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of POC validation exercise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import hamming_loss, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columndict={'Anti-Tax Avoidance Directive (ATAD)':'Anti-Tax Avoidance Directive (ATAD)', \n",
    "            'Controlled Foreign Corporation':'Controlled Foreign Corporation',\n",
    "            'Corporate Tax':'Corporate Tax', \n",
    "            'Country-by-country reporting':'Country-by-Country Reporting', \n",
    "            'Digital Tax':'Digital Tax',\n",
    "            'Double Tax Treaty ':'Double Tax Treaty', \n",
    "            'Economic substance':'Economic substance',\n",
    "            'EU Mandatory Disclosure Directive (DAC6)':'EU Mandatory Disclosure Directive (DAC6)', \n",
    "            'GAAR ':'General anti-avoidance rule (GAAR)', \n",
    "            'GST':'Goods and services tax (GST)',\n",
    "            'Interest Deductibility ':'Interest deductibility', \n",
    "            'Local File':'Local file', \n",
    "            'Management of a Tax Function':'(Management of a) Tax function',\n",
    "            'Mandatory disclosure rules':'Mandatory disclosure rules', \n",
    "            'Master File':'Master file', \n",
    "            'Multilateral Instrument':'Multilateral Instrument (MLI)',\n",
    "            'Mutual Agreement Procedure':'Mutual agreement procedure', \n",
    "            'Permanent Establishment':'Permanent Establishment (PE)',\n",
    "            'Principal purpose test':'Principal purpose test', \n",
    "            'Section 892':'Section 892', \n",
    "            'Sovereign Immunity':'Sovereign Immunity',\n",
    "            'Tax audit':'Tax audit', \n",
    "            'Tax compliance':'Tax compliance', \n",
    "            'Tax dispute':'Tax dispute',\n",
    "            'Tax Governance Framework':'Tax Governance Framework', \n",
    "            'VAT':'Value-added tax (VAT)', \n",
    "            'Withholding Tax':'Withholding Tax'}\n",
    "def imbibe_validation(filepath):\n",
    "    def func(item):\n",
    "        if(item=='Y'):\n",
    "            return 1\n",
    "        else: return 0\n",
    "    df_v1=pd.read_excel(filepath, skiprows=4, nrows=27).T\n",
    "    categories=df_v1.loc['Category']\n",
    "    df_v1=pd.DataFrame(df_v1.drop(index='Category').values, columns=categories)\n",
    "    df_v1=df_v1.applymap(func)\n",
    "    df_v1.rename(columns=columndict, inplace=True)\n",
    "    return df_v1\n",
    "\n",
    "def imbibe_country_validation(filepath):\n",
    "    countries_list=['Australia','Brazil','China','France','India','Japan','South Korea','Spain','UK','USA']\n",
    "    def func(item):\n",
    "        if(item=='Y'):\n",
    "            return 1\n",
    "        else: return 0\n",
    "    df_v1=pd.read_excel(filepath, skiprows=33, nrows=10).T\n",
    "    countries=df_v1.loc['Country']\n",
    "    df_v1=pd.DataFrame(df_v1.drop(index='Country').values, columns=countries)\n",
    "    df_v1=df_v1.applymap(func)\n",
    "#     df_v1.columns = countries_list\n",
    "    return df_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(df_prob, cutoff):\n",
    "    df_pred_cut=df_prob.copy()\n",
    "    df_pred_cut[df_pred_cut.columns]=(df_prob.values>cutoff)*1\n",
    "    return df_pred_cut\n",
    "\n",
    "def mean_score(scorer, df_probabilities, df_v1, df_v2, df_v3, cutoff, **kwargs):\n",
    "    total=0\n",
    "    total+=scorer(df_v1, make_predictions(df_probabilities, cutoff), **kwargs)\n",
    "    total+=scorer(df_v2, make_predictions(df_probabilities, cutoff), **kwargs)\n",
    "    total+=scorer(df_v3, make_predictions(df_probabilities, cutoff), **kwargs)\n",
    "    return total/3\n",
    "\n",
    "def mean_analyst_score(scorer, df_v1, df_v2, df_v3, **kwargs):\n",
    "    p1s=p2s=[df_v1, df_v2, df_v3]\n",
    "    total=0\n",
    "    count=0\n",
    "    for ii, p1 in enumerate(p1s):\n",
    "        for jj, p2 in enumerate(p2s):\n",
    "            if ii==jj:\n",
    "                pass\n",
    "            else:\n",
    "                total+=scorer(p1, p2, **kwargs)\n",
    "                count+=1\n",
    "    return total/count\n",
    "\n",
    "def cat_by_cat(valid, predict, topics):\n",
    "    df_results=pd.DataFrame({'f1':f1_score(valid, predict, average=None), \n",
    "                             'precision':precision_score(valid, predict, average=None), \n",
    "                             'recall':recall_score(valid, predict, average=None),\n",
    "                             'occurences':valid.apply(sum)\n",
    "                            })\n",
    "    display(df_results)\n",
    "    \n",
    "def read_subcat_files(a1_path, a2_path, a3_path, prob_path):\n",
    "    df_v1=imbibe_validation(a1_path)\n",
    "    df_v2=imbibe_validation(a2_path)\n",
    "    df_v3=imbibe_validation(a3_path)\n",
    "    df_probabilities=pd.read_csv(prob_path)\n",
    "    try:\n",
    "        df_probabilities.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    except: pass\n",
    "    df_probabilities=df_probabilities[df_v1.columns]\n",
    "    return df_v1, df_v2, df_v3, df_probabilities\n",
    "\n",
    "def read_country_files(a1_path, a2_path, a3_path, country_path):\n",
    "    df_v1=imbibe_country_validation(a1_path)\n",
    "    df_v2=imbibe_country_validation(a2_path)\n",
    "    df_v3=imbibe_country_validation(a3_path)\n",
    "    df_countries=pd.read_csv(country_path, index_col=0)\n",
    "    return df_v1, df_v2, df_v3, df_countries\n",
    "\n",
    "def print_hamm_f1_prec_rec(y_true, y_pred, title):\n",
    "    print('{} hamming: {:.4f}, F1: {:.4f}, Precision: {:.4f}, Recall: {:.4f}'\n",
    "          .format(title, hamming_loss(y_true, y_pred), f1_score(y_true, y_pred, average='weighted'), \n",
    "                  precision_score(y_true, y_pred, average='weighted'), recall_score(y_true, y_pred, average='weighted') ))\n",
    "    return f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "def scores_at_each_cutoff(start, end, step, df_v1, df_v2, df_v3, df_probabilities):\n",
    "    for cutoff in np.arange(start, end, step):\n",
    "        print('cutoff: {:.1f}, Hamming: {:.4f},  F1: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, hits/doc:{:.2f}'.format(\n",
    "            cutoff,\n",
    "            mean_score(hamming_loss, df_probabilities, df_v1, df_v2, df_v3, cutoff), \n",
    "            mean_score(f1_score, df_probabilities, df_v1, df_v2, df_v3, cutoff, average='weighted'), \n",
    "            mean_score(precision_score, df_probabilities, df_v1, df_v2, df_v3, cutoff, average='weighted'), \n",
    "            mean_score(recall_score, df_probabilities, df_v1, df_v2, df_v3, cutoff, average='weighted'), \n",
    "            np.sum(make_predictions(df_probabilities, cutoff).apply(sum))/20 ))\n",
    "\n",
    "def label_comparison(df_v1, df_v2, df_v3, df_pred_cut, main_model_cutoff):\n",
    "    for df, agent in zip([df_v1, df_v2, df_v3, df_pred_cut], ['analyst 1', 'analyst 2', 'analyst 3', 'computer @ '+str(main_model_cutoff)]):\n",
    "        print('Number of labels tagged by {}: {}'.format(agent, np.sum(df.apply(sum))))\n",
    "        \n",
    "def analyst_summary(df_v1, df_v2, df_v3):\n",
    "    print('\\n', 'Intra-analyst scores. Hamming: {:.4f},  F1: {:.4f}, Precision: {:.4f}, Recall: {:.4f}'.format(\n",
    "        mean_analyst_score(hamming_loss, df_v1, df_v2, df_v3), \n",
    "        mean_analyst_score(f1_score, df_v1, df_v2, df_v3, average='weighted'), \n",
    "        mean_analyst_score(precision_score, df_v1, df_v2, df_v3, average='weighted'), \n",
    "        mean_analyst_score(recall_score, df_v1, df_v2, df_v3, average='weighted') ))\n",
    "    \n",
    "def analyze_subcat_results(a1_path, a2_path, a3_path, prob_path, main_model_cutoff=0.3, verbose=True):\n",
    "    '''Takes in analyst probabilities and returns summary statistics on superset scores.'''\n",
    "    \n",
    "    df_v1, df_v2, df_v3, df_probabilities=read_subcat_files(a1_path, a2_path, a3_path, prob_path)\n",
    "    \n",
    "    df_pred_cut=make_predictions(df_probabilities, main_model_cutoff)\n",
    "    #Potentially load fine-tuned predictions also\n",
    "    \n",
    "    if verbose:\n",
    "        scores_at_each_cutoff(0.1, 1.0, 0.1, df_v1, df_v2, df_v3, df_probabilities)\n",
    "        label_comparison(df_v1, df_v2, df_v3, df_pred_cut, main_model_cutoff)\n",
    "        analyst_summary(df_v1, df_v2, df_v3)\n",
    "        print('\\n')\n",
    "        \n",
    "    for ii in range(1,4):\n",
    "        df_vote=(df_v1+df_v2+df_v3)//ii\n",
    "        if verbose:\n",
    "            print('Total number of categories identified', np.sum(df_vote.apply(sum)))\n",
    "        title='Majority of '+ str(ii) \n",
    "        if(ii==1):\n",
    "            score=print_hamm_f1_prec_rec(df_vote>0, df_pred_cut, title)\n",
    "        elif(verbose):\n",
    "            print_hamm_f1_prec_rec(df_vote>0, df_pred_cut, title)\n",
    "    if verbose:\n",
    "        print('\\n')\n",
    "        print('Superset breakdown @ cut-off {}'.format(main_model_cutoff))\n",
    "        cat_by_cat(((df_v1+df_v2+df_v3)//1)>0, df_pred_cut, df_v1.columns)\n",
    "    return score\n",
    "    \n",
    "def analyze_country_results(a1_path, a2_path, a3_path, country_path, verbose=True):\n",
    "    '''Takes in analyst probabilities and returns summary statistics on superset scores.'''\n",
    "    \n",
    "    df_v1, df_v2, df_v3, df_countries=read_country_files(a1_path, a2_path, a3_path, country_path)\n",
    "    if verbose:\n",
    "        analyst_summary(df_v1, df_v2, df_v3)\n",
    "        print('\\n')\n",
    "        \n",
    "    for ii in range(1,4):\n",
    "        df_vote=(df_v1+df_v2+df_v3)//ii\n",
    "        if verbose:\n",
    "            print('Total number of countries identified', np.sum(df_vote.apply(sum)))\n",
    "        title='Majority of '+ str(ii)\n",
    "        if(ii==1):\n",
    "            score=print_hamm_f1_prec_rec(df_vote>0, df_countries, title)\n",
    "        elif(verbose):\n",
    "            print_hamm_f1_prec_rec(df_vote>0, df_countries, title)\n",
    "        \n",
    "    if verbose:\n",
    "        print('\\n')\n",
    "        print('Superset breakdown')\n",
    "        cat_by_cat(((df_v1+df_v2+df_v3)//1)>0, df_countries, df_v1.columns)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_results(a1_path, a2_path, a3_path, prob_path, country_path, main_model_cutoff=0.3, verbose=True):\n",
    "    subcat_score=analyze_subcat_results(a1_path, a2_path, a3_path, prob_path, main_model_cutoff=main_model_cutoff, verbose=verbose)\n",
    "    country_score=analyze_country_results(a1_path, a2_path, a3_path, country_path, verbose=verbose)\n",
    "    return subcat_score, country_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble(bertpath, v1path, outpath):\n",
    "    df_bert=pd.read_csv(bertpath)\n",
    "    df_v1=pd.read_csv(v1path)\n",
    "    df_v1.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    df_mixed=(df_bert+df_v1)/2\n",
    "    df_mixed.to_csv(outpath), index=False)\n",
    "    \n",
    "generate_ensemble('./data/test1_BERT_results.csv', './data/test1_probabilities.csv', './data/test1_bert_modelv1.csv')\n",
    "generate_ensemble('./data/test2_BERT_results.csv', './data/test2_probabilities.csv', './data/test2_bert_modelv1.csv')\n",
    "generate_ensemble('./data/test3_BERT_results.csv', './data/test3_probabilities.csv', './data/test3_bert_modelv1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results below\n",
    "## Overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Intra-analyst scores. Hamming: 0.1226,  F1: 0.6351, Precision: 0.7516, Recall: 0.6761\n",
      "<function hamming_loss at 0x000000F0379E1E18> 0.0734567901234568\n",
      "<function hamming_loss at 0x000000F0379E1E18> 0.16913580246913582\n",
      "<function hamming_loss at 0x000000F0379E1E18> 0.06604938271604938\n",
      "<function f1_score at 0x000000F0379E19D8> 0.6940646552370409\n",
      "<function f1_score at 0x000000F0379E19D8> 0.5622198964619266\n",
      "<function f1_score at 0x000000F0379E19D8> 0.6973807586734708\n",
      "<function precision_score at 0x000000F0379E1BF8> 0.7579834054834055\n",
      "<function precision_score at 0x000000F0379E1BF8> 0.420365328072095\n",
      "<function precision_score at 0x000000F0379E1BF8> 0.7034492974572558\n",
      "<function recall_score at 0x000000F0379E1C80> 0.6944444444444444\n",
      "<function recall_score at 0x000000F0379E1C80> 0.9398496240601504\n",
      "<function recall_score at 0x000000F0379E1C80> 0.7668711656441718\n",
      "\n",
      " Intra-analyst scores. Hamming: 0.1029,  F1: 0.6512, Precision: 0.6273, Recall: 0.8004\n",
      "Total number of categories identified per doc 12.9 4.15\n",
      "Superset score  hamming: 0.1031, F1: 0.6274, Precision: 0.6204, Recall: 0.7278\n",
      "Superset score  hamming: 0.1123, F1: 0.5714, Precision: 0.5267, Recall: 0.7519\n",
      "Superset score  hamming: 0.1086, F1: 0.6062, Precision: 0.5941, Recall: 0.7239\n",
      "mean score 0.6016824146963816\n",
      "Superset score  hamming: 0.1395, F1: 0.6462, Precision: 0.9201, Recall: 0.5272\n",
      "\n",
      "\n",
      "Superset breakdown @ cut-off 0.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anti-Tax Avoidance Directive (ATAD)</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Controlled Foreign Corporation</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corporate Tax</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country-by-Country Reporting</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Digital Tax</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Double Tax Treaty</th>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economic substance</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU Mandatory Disclosure Directive (DAC6)</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General anti-avoidance rule (GAAR)</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goods and services tax (GST)</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interest deductibility</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Local file</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Management of a) Tax function</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mandatory disclosure rules</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Master file</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multilateral Instrument (MLI)</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mutual agreement procedure</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Permanent Establishment (PE)</th>\n",
       "      <td>0.848485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Principal purpose test</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Section 892</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sovereign Immunity</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tax audit</th>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tax compliance</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tax dispute</th>\n",
       "      <td>0.745098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tax Governance Framework</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Value-added tax (VAT)</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Withholding Tax</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                f1  precision    recall  \\\n",
       "Category                                                                  \n",
       "Anti-Tax Avoidance Directive (ATAD)       0.800000   1.000000  0.666667   \n",
       "Controlled Foreign Corporation            0.285714   1.000000  0.166667   \n",
       "Corporate Tax                             0.571429   0.952381  0.408163   \n",
       "Country-by-Country Reporting              0.428571   1.000000  0.272727   \n",
       "Digital Tax                               0.628571   1.000000  0.458333   \n",
       "Double Tax Treaty                         0.697674   0.789474  0.625000   \n",
       "Economic substance                        0.600000   0.562500  0.642857   \n",
       "EU Mandatory Disclosure Directive (DAC6)  0.857143   1.000000  0.750000   \n",
       "General anti-avoidance rule (GAAR)        0.823529   0.777778  0.875000   \n",
       "Goods and services tax (GST)              0.700000   1.000000  0.538462   \n",
       "Interest deductibility                    0.800000   1.000000  0.666667   \n",
       "Local file                                0.800000   0.857143  0.750000   \n",
       "(Management of a) Tax function            0.692308   0.818182  0.600000   \n",
       "Mandatory disclosure rules                0.400000   0.750000  0.272727   \n",
       "Master file                               0.941176   1.000000  0.888889   \n",
       "Multilateral Instrument (MLI)             0.750000   1.000000  0.600000   \n",
       "Mutual agreement procedure                0.923077   0.923077  0.923077   \n",
       "Permanent Establishment (PE)              0.848485   1.000000  0.736842   \n",
       "Principal purpose test                    0.857143   1.000000  0.750000   \n",
       "Section 892                               0.666667   0.500000  1.000000   \n",
       "Sovereign Immunity                        0.333333   0.500000  0.250000   \n",
       "Tax audit                                 0.717949   0.875000  0.608696   \n",
       "Tax compliance                            0.520000   1.000000  0.351351   \n",
       "Tax dispute                               0.745098   1.000000  0.593750   \n",
       "Tax Governance Framework                  0.444444   1.000000  0.285714   \n",
       "Value-added tax (VAT)                     0.545455   1.000000  0.375000   \n",
       "Withholding Tax                           0.736842   0.875000  0.636364   \n",
       "\n",
       "                                          occurences  \n",
       "Category                                              \n",
       "Anti-Tax Avoidance Directive (ATAD)                3  \n",
       "Controlled Foreign Corporation                    12  \n",
       "Corporate Tax                                     49  \n",
       "Country-by-Country Reporting                      11  \n",
       "Digital Tax                                       24  \n",
       "Double Tax Treaty                                 24  \n",
       "Economic substance                                14  \n",
       "EU Mandatory Disclosure Directive (DAC6)           4  \n",
       "General anti-avoidance rule (GAAR)                 8  \n",
       "Goods and services tax (GST)                      13  \n",
       "Interest deductibility                            12  \n",
       "Local file                                         8  \n",
       "(Management of a) Tax function                    15  \n",
       "Mandatory disclosure rules                        22  \n",
       "Master file                                        9  \n",
       "Multilateral Instrument (MLI)                     10  \n",
       "Mutual agreement procedure                        13  \n",
       "Permanent Establishment (PE)                      19  \n",
       "Principal purpose test                             4  \n",
       "Section 892                                        1  \n",
       "Sovereign Immunity                                 4  \n",
       "Tax audit                                         23  \n",
       "Tax compliance                                    37  \n",
       "Tax dispute                                       32  \n",
       "Tax Governance Framework                          14  \n",
       "Value-added tax (VAT)                             16  \n",
       "Withholding Tax                                   22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Intra-analyst scores. Hamming: 0.0622,  F1: 0.8233, Precision: 0.8476, Recall: 0.8336\n",
      "<function hamming_loss at 0x000000F0379E1E18> 0.058333333333333334\n",
      "<function hamming_loss at 0x000000F0379E1E18> 0.08\n",
      "<function hamming_loss at 0x000000F0379E1E18> 0.04833333333333333\n",
      "<function f1_score at 0x000000F0379E19D8> 0.8159108590410648\n",
      "<function f1_score at 0x000000F0379E19D8> 0.769666490820337\n",
      "<function f1_score at 0x000000F0379E19D8> 0.8488010038386763\n",
      "<function precision_score at 0x000000F0379E1BF8> 0.8507382073708604\n",
      "<function precision_score at 0x000000F0379E1BF8> 0.6302554387961627\n",
      "<function precision_score at 0x000000F0379E1BF8> 0.8076265389876882\n",
      "<function recall_score at 0x000000F0379E1C80> 0.7959183673469388\n",
      "<function recall_score at 0x000000F0379E1C80> 1.0\n",
      "<function recall_score at 0x000000F0379E1C80> 0.9069767441860465\n",
      "\n",
      " Intra-analyst scores. Hamming: 0.0622,  F1: 0.8115, Precision: 0.7629, Recall: 0.9010\n",
      "Total number of countries identified per doc 2.2333333333333334 1.6\n",
      "Superset score  hamming: 0.0900, F1: 0.7517, Precision: 0.9205, Recall: 0.6567\n",
      "\n",
      "\n",
      "Superset breakdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brazil</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japan</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Korea</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>0.620690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   f1  precision    recall  occurences\n",
       "Country                                               \n",
       "Australia    0.769231   0.909091  0.666667          15\n",
       "Brazil       1.000000   1.000000  1.000000           8\n",
       "China        0.900000   0.900000  0.900000          10\n",
       "France       0.666667   0.818182  0.562500          16\n",
       "India        0.937500   1.000000  0.882353          17\n",
       "Japan        0.823529   0.777778  0.875000           8\n",
       "South Korea  0.545455   0.750000  0.428571           7\n",
       "Spain        0.800000   1.000000  0.666667           9\n",
       "UK           0.620690   1.000000  0.450000          20\n",
       "USA          0.648649   0.923077  0.500000          24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "def concat_per_pax(ex1dir, ex2dir, ex3dir):\n",
    "    df_ex1=imbibe_validation(ex1dir)\n",
    "    df_ex2=imbibe_validation(ex2dir)\n",
    "    df_ex3=imbibe_validation(ex3dir=)\n",
    "    df_full=pd.concat([pd.concat([df_ex1, df_ex2], axis=0), df_ex3], axis=0)\n",
    "    return df_full\n",
    "\n",
    "def concat_ctry_per_pax(ex1dir, ex2dir, ex3dir):\n",
    "    df_ex1=imbibe_country_validation(ex1dir)\n",
    "    df_ex2=imbibe_country_validation(ex2dir)\n",
    "    df_ex3=imbibe_country_validation(ex3dir)\n",
    "    df_full=pd.concat([pd.concat([df_ex1, df_ex2], axis=0), df_ex3], axis=0)\n",
    "    return df_full\n",
    "\n",
    "def superset_analyst_score(scorer, df_v1, df_v2, df_v3, **kwargs):\n",
    "    p1s=[df_v1, df_v2, df_v3]\n",
    "    p2s=[((df_v2+df_v3)//1)>1, ((df_v1+df_v3)//1)>1, ((df_v2+df_v1)//1)>1 ]\n",
    "    total=0\n",
    "    count=0\n",
    "    for p1, p2 in zip(p1s, p2s):\n",
    "        print(scorer, scorer(p2, p1, **kwargs))\n",
    "        total+=scorer(p2, p1, **kwargs)\n",
    "        count+=1\n",
    "    return total/count\n",
    "\n",
    "def analyst_superset_summary(df_v1, df_v2, df_v3):\n",
    "    print('\\n', 'Intra-analyst scores. Hamming: {:.4f},  F1: {:.4f}, Precision: {:.4f}, Recall: {:.4f}'.format(\n",
    "        superset_analyst_score(hamming_loss, df_v1, df_v2, df_v3), \n",
    "        superset_analyst_score(f1_score, df_v1, df_v2, df_v3, average='weighted'), \n",
    "        superset_analyst_score(precision_score, df_v1, df_v2, df_v3, average='weighted'), \n",
    "        superset_analyst_score(recall_score, df_v1, df_v2, df_v3, average='weighted') ))    \n",
    "\n",
    "def individual_superset_analyst_score(scorer, df_v1, df_v2, df_v3, df_model, **kwargs):\n",
    "    p1s=[df_v1, df_v2, df_v3]\n",
    "    p2s=[((df_v2+df_v3)//1)>1, ((df_v1+df_v3)//1)>1, ((df_v2+df_v1)//1)>1 ]\n",
    "    analyst_scores=[]\n",
    "    model_scores=[]\n",
    "    for p1, p2 in zip(p1s, p2s):\n",
    "        analyst_scores.append(list(scorer(p2, p1, **kwargs)))\n",
    "        model_scores.append(list(scorer(p2, df_model, **kwargs)))\n",
    "    return [item for onelist in analyst_scores for item in onelist ], [item for onelist in model_scores for item in onelist]\n",
    "    \n",
    "def results_combiner(ex1a1, ex2a1, ex3a1, ex1a2, ex2a2, ex3a2, ex1a3, ex2a3, ex3a3, \n",
    "                     cattest1, cattest2, cattest3, ctrytest1, ctrytest2, ctrytest3, main_model_cutoff):\n",
    "    df_analyst_1=concat_per_pax(ex1a1, ex2a1, ex3a1)\n",
    "    df_analyst_2=concat_per_pax(ex1a2, ex2a2, ex3a2)\n",
    "    df_analyst_3=concat_per_pax(ex1a3, ex2a3, ex3a3)\n",
    "#     df_analyst_1.to_csv('./combined_analyst1.csv')  \n",
    "    df_probabilities=pd.concat([pd.concat([pd.read_csv(cattest1)[df_analyst_1.columns], \n",
    "                                           pd.read_csv(cattest2)[df_analyst_1.columns]], axis=0), \n",
    "                                pd.read_csv(cattest3)[df_analyst_1.columns]], axis=0)\n",
    "    df_pred_cut=make_predictions(df_probabilities, main_model_cutoff)\n",
    "    analyst_summary(df_analyst_1, df_analyst_2, df_analyst_3)\n",
    "    df_vote=(df_analyst_1+df_analyst_2+df_analyst_3)//1\n",
    "    analyst_superset_summary(df_analyst_1, df_analyst_2, df_analyst_3)\n",
    "    print('Total number of categories identified per doc', np.sum(df_vote.apply(sum))/60, np.sum(df_pred_cut.apply(sum))/60)\n",
    "    title='Superset score ' \n",
    "    p2s=[((df_analyst_2+df_analyst_3)//1)>1, ((df_analyst_1+df_analyst_3)//1)>1, ((df_analyst_2+df_analyst_1)//1)>1 ]\n",
    "    score=0\n",
    "    for p2 in p2s:\n",
    "        score+=print_hamm_f1_prec_rec(p2, df_pred_cut, title)\n",
    "    print('mean score', score/3)\n",
    "    score=print_hamm_f1_prec_rec(df_vote>0, df_pred_cut, title)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Superset breakdown @ cut-off {}'.format(main_model_cutoff))\n",
    "    cat_by_cat(df_vote>0, df_pred_cut, df_analyst_1.columns)\n",
    "    \n",
    "#     analyst_scores, model_scores=individual_superset_analyst_score(f1_score, df_analyst_1, df_analyst_2, df_analyst_3, df_pred_cut, average=None)\n",
    "#     return analyst_scores, model_scores\n",
    "\n",
    "    '''This section for countries'''\n",
    "    df_analyst_1=concat_ctry_per_pax(ex1dir, ex2dir, ex3dir, 0)\n",
    "    df_analyst_2=concat_ctry_per_pax(ex1dir, ex2dir, ex3dir, 1)\n",
    "    df_analyst_3=concat_ctry_per_pax(ex1dir, ex2dir, ex3dir, 2)\n",
    "\n",
    "    df_countries=pd.concat([pd.concat([pd.read_csv(ctrytest1, index_col=0), \n",
    "                                           pd.read_csv(ctrytest2, index_col=0)], axis=0), \n",
    "                                pd.read_csv(ctrytest3, index_col=0)], axis=0)\n",
    "    analyst_summary(df_analyst_1, df_analyst_2, df_analyst_3)\n",
    "    df_vote=((df_analyst_1+df_analyst_2+df_analyst_3)//1)>0\n",
    "    analyst_superset_summary(df_analyst_1, df_analyst_2, df_analyst_3)\n",
    "    print('Total number of countries identified per doc', np.sum(df_vote.apply(sum))/60, np.sum(df_countries.apply(sum))/60)\n",
    "    title='Superset score ' \n",
    "    score=print_hamm_f1_prec_rec(df_vote, df_countries, title)\n",
    "    print('\\n')\n",
    "    print('Superset breakdown')\n",
    "    cat_by_cat(df_vote, df_countries, df_analyst_1.columns)    \n",
    "    \n",
    "results_combiner('./data/POC1_DarrenYang.xlsm', './data/POC2_DarrenYang.xlsm', './data/POC3_DarrenYang.xlsm',\n",
    "                 './data/POC1_KirstinGallagher.xlsm', './data/POC2_KirstinGallagher.xlsm', './data/POC3_KirstinGallagher.xlsm',\n",
    "                 './data/POC1_RichardMadden.xlsm', './data/POC2_RichardMadden.xlsm', './data/POC3_RichardMadden.xlsm',\n",
    "                 './data/test1_bert_modelv1.csv', './data/test2_bert_modelv1.csv', './data/test3_bert_modelv1.csv', \n",
    "                 './data/test1_country_predictions.csv', './data/test2_country_predictions.csv', \n",
    "                 './data/test3_country_predictions.csv', 0.6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3env)",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
